<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>爬虫学习路线</title>
    <url>/2020/08/16/1/</url>
    <content><![CDATA[<h1 id="爬虫学习路线"><a href="#爬虫学习路线" class="headerlink" title="爬虫学习路线"></a>爬虫学习路线</h1>]]></content>
      <categories>
        <category>-python 爬虫</category>
      </categories>
      <tags>
        <tag>爬虫学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫笔记——基础篇（一）</title>
    <url>/2020/08/16/1.1/</url>
    <content><![CDATA[<h1 id="爬虫学习笔记"><a href="#爬虫学习笔记" class="headerlink" title="爬虫学习笔记"></a>爬虫学习笔记</h1><h2 id="第一章-网络请求"><a href="#第一章-网络请求" class="headerlink" title="第一章 网络请求"></a>第一章 网络请求</h2><h3 id="一-HTTP协议和chrome抓包工具"><a href="#一-HTTP协议和chrome抓包工具" class="headerlink" title="一. HTTP协议和chrome抓包工具"></a>一. HTTP协议和chrome抓包工具</h3><h3 id="什么是http和https协议："><a href="#什么是http和https协议：" class="headerlink" title="什么是http和https协议："></a>什么是http和https协议：</h3><p>​    HTTP协议：全称是HwperTextTransfer Protocol，中文意思是超文本传输协议，是一种发布和接收HTML页面的方法。服务器端口号是80端口。<br>​            HTTPS协议；是HTTP协议的加密版本，在HTTP下加入了SSL层，颜务器端口号是443端口。</p>
<h3 id="在浏览器中发送一个http请求的过程："><a href="#在浏览器中发送一个http请求的过程：" class="headerlink" title="在浏览器中发送一个http请求的过程："></a>在浏览器中发送一个http请求的过程：</h3><p>​            1.当用户在浏览器的地址栏中输入一个URL并按回车键之后，浏览器会向HTTP服务器发送HTTP请求。HTTP请求主要分为“Get”和“Post”两种方法。<br>​            2.当我们在浏览器输入URL http:/<a href="http://www.baidu.com的时候,浏览器发送一个request请求去获取htp//www.baidu.com%E7%9A%84htm%E6%96%87%E4%BB%B6%EF%BC%8C%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%8A%8AResponse%E6%96%87%E4%BB%B6%E5%AF%B9%E8%B1%A1%E5%8F%91%E9%80%81%E5%9B%9E%E7%BB%99%E6%B5%8F%E8%A7%88%E5%99%A8%E3%80%82">www.baidu.com的时候，浏览器发送一个Request请求去获取htp://www.baidu.com的htm文件，服务器把Response文件对象发送回给浏览器。</a><br>​            3.浏览器分析Response中的HTML，发现其中引用了很多其他文件，比如mages文件，CSS文件，JS文件。刻货器会自动再次发送Request去获取图片，CSS文件，或者JS文件。<br>​            4.当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来了。</p>
<h3 id="url详解"><a href="#url详解" class="headerlink" title="url详解"></a>url详解</h3><p>URL是Unifora Resource Locator的简写，统一资源定位符。一个URL由以下几部分组成：</p>
<p>​            <strong>schene://host:port/path/？query-string-xxx#anchor</strong></p>
<p>scheme：代表的是访问的协议，一般为http或者https以及代p等。<br>        host：主机名，域名，比如ww.baidu.com。<br>        port：端口号。当你访问一个网站的时候，浏览器默认使用80端口。<br>        path：查找路径。比如：ww.jianshu.com/trending/now，后面的trending/now 就是path。<br>        query-string：查询字符串，比如：wwr.baidu.con/s7we-python，后面的wd-oython 就是查询字符串。<br>        anchor：锚点，后台一般不用管，前端用来做页面定位的。<br>        在浏览器中请求一个url，浏览器会对这个url进行一个编码。除英文字母，数字和部分符号外，其他的全部使用百分号+十六进制码值进行编码。</p>
<h3 id="常用的请求方法"><a href="#常用的请求方法" class="headerlink" title="常用的请求方法"></a>常用的请求方法</h3><p>在Http协议中，定义了八种请求方法。这里介绍两种常用的请求方法，分别是set请求和post请求。<br>        1.get请求：一般情况下，只从服务器获取数据下来，并不会对服务器资源产生任何影响的时候会使用get 请求。<br>        2.post 请求：向服务器发送数据（登录）、上传文件等，会对服务器资源产生影响的时候会使用post请求。<br>以上是在网站开发中常用的两种方法。并且一般情况下都会遵循使用的原则。但是有的网站和服务器为了做反爬虫机制，也经常会不按常理出牌，有可能一个应该使用aet方法的请求就一定要改成post请求，这个要视情况而定。</p>
<h3 id="请求头常见参数"><a href="#请求头常见参数" class="headerlink" title="请求头常见参数"></a>请求头常见参数</h3><p>在http协议中，向服务器发送一个请求，数据分为三部分，第一个是把数据放在ul中，第二个是把数据放在boay中（在post请求中），第三个就是把数据放在head中。这里介绍在网络虫中经常会用到的一些请求头参数：<br>        1.user-Agent：浏览器名称。这个在网络爬虫中经常会被使用到。请求一个网页的时候，服务器通过这个参数就可以知道这个请求是由哪种刻览器发送的。如果我们是通过爬虫发送请求，那么我们user-Agent 就是aythcn，这对于那些有反爬虫机制的网站来说，可以轻易的判断你这个请求是爬虫。因此我们要经常设置这个植为一些浏览器的值，来伪装我们的爬虫。<br>        2.Referer；表明当前这个请求是从哪个ur1过来的。这个一般也可以用来做反爬虫技术。如果不是从指定页面过来的，那么就不做相关的响应。<br>        3.cookie:http 协议是无状态的。也就是同一个人发送了两次请求，服务器没有能力知道这两个请求是否来自同一个人。因此这时候就用cookie来做标识。一般如果想要做登录后才能访问的网站，那么就需要发送cookie信息了。</p>
<h3 id="常见响应状态码"><a href="#常见响应状态码" class="headerlink" title="常见响应状态码"></a>常见响应状态码</h3><p>1.200：请求正常，服务器正常的返回数据。<br>        2.301：永久重定向。比如在访问ww.jingdong.com的时候会重定向到ww.jc.com。<br>        3.302：临时重定向。比如在访问一个需要登录的页面的时候，而此时没有登录，那么就会重定向到登录页面。<br>        4.400：请求的ur1在服务器上找不到。换句话说就是请求ur1错误。<br>        5.403：服务器拒绝访问，权限不够。<br>        6.500：服务器内部错误。可能是服务器出现bug了。</p>
<h2 id="二-urllib库简单用法"><a href="#二-urllib库简单用法" class="headerlink" title="二. urllib库简单用法"></a>二. urllib库简单用法</h2><ul>
<li><p><code>request.urlopen()</code></p>
<p>ur1：请求的url。<br>data：请求的data，如果设置了这个值，那么将变成post请求。<br>返回值：返回值是一个http.client.HTTPResponse对象，这个对象是一个类文件句柄对象。<br>有read（size）、readline（读取一行）、readlines（读取多行）以及getcode(获取响应码) 等方法。</p>
</li>
<li><p><code>request.urlretrieve(网页的url, 下载名字)</code></p>
<pre><code>把网页上的一个文件保存到本地 </code></pre>
</li>
<li><p><code>parse.urlencode(key：value)</code><br>如果url有中文或者其他特殊字符，那么浏览器会自动给我们进行编码，但是通过爬虫来访问的时候，就必须用手动来进行编码。 这个方法可以将一个字典编码成key=value形式，且都用ascii码表示</p>
<p>例如：这个方法可以将中文编码成%0x形式表示=</p>
<p>tips：这种方法可以直接用 .encode() 方法代替。</p>
</li>
<li><p><code>parse.parse_qs()</code><br>这个相当于urlencode的反函数，解码</p>
</li>
<li><p><code>parse.urlparse()</code> 和 <code>parse.urlsplit()</code></p>
<p>顾名思义，这2个函数是对url进行解析</p>
<p>不同点，前者urlparse 会多返回一个params的属性参数。<br>这个params用的很少</p>
</li>
</ul>
<ul>
<li><p>request库中的Request类。  request.Request<br><strong>如果想要在请求的时候增加一些请求头，那么则必须使用request.Request类来实现</strong>。</p>
</li>
<li><p>ProxyHandler 处理器（代理设置）</p>
<ul>
<li>代理的原理：在请求目的网站之前，先请求代理服务器，然后让代理服务器去请求目的网站，代理服务器拿到目的网站的数据后，再转发给我们的代码。</li>
<li><code>http://httpbin.org</code>：这个网站可以方便的查看http请求的一些参数</li>
</ul>
<p>很多网站会检测某一段时某个IP的访问次数（通过流量统计，系统日志等），如果访问次数多的不像正常人，它会禁止这个IP的访问。<br>所以我们可以设置一些代理服务器，每隔一段时间换一个代理，就算IP被禁止，依然可以换个IP继续爬取。</p>
<p><strong>在代码中使用代理：</strong></p>
<pre><code>*使用“ur11ib.request.ProxyHandler”，传入一个代理，这个代理是一个字典，字典的key依赖于代理服务器能够接收的类型，一般是‘http或者https，值是ip:port&#39;
*使用上一步创建的“handler”，以request.build_opener”创建一个opener对象。
*使用上一步创建的opener，调用‘open’函数，发起请求。</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">url = <span class="string">&#x27;http://httpbin.org/ip&#x27;</span></span><br><span class="line"><span class="comment"># 用ProxyHandler传入代理构建一个handler</span></span><br><span class="line">handler=request.ProxyHandler(&#123;<span class="string">&quot;http&quot;</span>:<span class="string">&quot;27.43.190.180:9999&quot;</span>&#125;)</span><br><span class="line"><span class="comment"># 使用上面创建的handler构建一个opener</span></span><br><span class="line">opener = request.build_opener(handler)</span><br><span class="line"><span class="comment"># 用opener去发送一个请求</span></span><br><span class="line">response = opener.open(url)</span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure>

<p><strong>常用的代理</strong></p>
<p>​        西刺免费代理IP:<code>http://www.xicidaili.com/</code><br>​        快代理：<code>http://www.kuaidaili.com/</code><br>​        代理云：<code>http://www.dailiyun.com/</code></p>
</li>
</ul>
<h3 id="什么是cookie："><a href="#什么是cookie：" class="headerlink" title="什么是cookie："></a>什么是cookie：</h3><p>在网站中，htp请求是无状态的。也就是说即使第一次和服务器连接后并且登录成功后，第二次请求服务器依然不能知道当前请求是哪个用户。cookie的出现就是为了解决这个问题，第一次登录后服务器返回一些数据（cookie）给浏览器，然后浏览器保存在本地，当该用户发送第二次请求的时候，就会自动的把上次请求存储的cookie数据自动的携带给服务器，服务器通过浏览器携带的数据就能判断当前用户是哪个了。cookte 存储的数据量有限，不同的浏览器有不同的存储大小，但一般不超过4KB。因此使用cookie 只能存储一些小量的数据。</p>
<ul>
<li><p><strong>cookie的格式：</strong></p>
<p><code>Set-Cookie:NAME=VALUE:Expires/Max-age=DATE:Path=PATH:Domain-DOHAIN_NAUIE：SECURE</code></p>
</li>
</ul>
<p>  参数意义：<br>          NAME:cookie的名字。VALUE:cookie的值。<br>          Expires:cookie的过期时间。<br>          Path:cookie作用的路径。<br>          Domain:cookie作用的域名。<br>          SECURE：是否只在https协议下起作用。     </p>
<ul>
<li><strong>对Cookie的操作（基于urllib）</strong></li>
</ul>
<ol>
<li><p>cookielib库和HTTPCookieProcessor 模拟登陆</p>
<p>Cookie是指网站服务器为了辨别用户身份和进行session跟踪。而储存在用户浏览器上的文本文件，cookie可以保持登陆信息到用户等下次与服务器的session</p>
<ul>
<li><p>方法一：直接把网页登陆后的Cookie复制到请求头上传进去。</p>
</li>
<li><p><strong>方法二：爬虫自动登陆授权页面</strong></p>
<p>用到的模块：<strong>http.cookiejar模块</strong>和HTTPCookieProcessor</p>
<p>用到的类：<br><strong>CookieJar</strong>： 管理HTTP cookie值，存储HTTP请求生成的cookie，向传出的HTTP请求添加cookie的对象。整个cookie都存储再内存中，对CookieJar实例进行垃圾回收后，cookie也将丢失<br><strong>FileCookieJar(filename, delayload=None, policy=None)**：由CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储在文件当中。filename是存储cookie的文件名。delayload为true时，支持延时访问文件，即只有在需要的时候猜读取文件或者再文件中存储数据<br>**MozilaCookieJar，LWPCookieJar</strong>：都是由FileCookieJar派生而来。不同点：格式与兼容有区别。主要用MozilaCookieJar 。该类创建与Mozila浏览器cookie.txt兼容的FileCookieJar实例。</p>
</li>
</ul>
</li>
<li><p>保存Cookie到本地<br>保存cookie到本地，可以使用cookiejar的sava函数，并且需要指定一个文件名。<br>然后对应还有一个load函数<br>这两个函数的ignore_discard参数是指 加载或者保存过期的cookie</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cookiejar = MozillaCookieJar(<span class="string">&quot;cookie.txt&quot;</span>) <span class="comment"># 火狐公司的cookie格式，文件名为cookie</span></span><br><span class="line">handler = request.HTTPCokkieProcessor(cookiejar)</span><br><span class="line">opener = request.build_opener(handler)</span><br><span class="line">headers = &#123;...&#125;</span><br><span class="line">req = request.Request(url, headers)</span><br><span class="line">resp = opener.open(req)</span><br><span class="line">print(resp.read())</span><br><span class="line">cookiejar.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>爬虫学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫学习笔记——基础篇（二）</title>
    <url>/2020/08/16/1.2/</url>
    <content><![CDATA[<h1 id="爬虫学习笔记"><a href="#爬虫学习笔记" class="headerlink" title="爬虫学习笔记"></a>爬虫学习笔记</h1><h2 id="第二章-数据提取"><a href="#第二章-数据提取" class="headerlink" title="第二章 数据提取"></a>第二章 数据提取</h2><h3 id="一-XPath-语法和lxml模块"><a href="#一-XPath-语法和lxml模块" class="headerlink" title="一. XPath 语法和lxml模块"></a>一. XPath 语法和lxml模块</h3><p>xpath:(XML Path Language) 是一门在XML和HTML文档中查找信息的语言，可用来在XML和HTML文档中对元素和属性进行遍历<br>xpath 开发工具：<br>    chrome插件 xpath Helper<br>    firefox插件 try xpath</p>
<ul>
<li><p>XPath 语法：</p>
<ol>
<li>选取节点：<br>XPath使用路径表达式来选取XML文档中的节点或者节点集。这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似</li>
</ol>
<table>
<thead>
<tr>
<th align="center">表达式</th>
<th>描述</th>
<th>示例</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td align="center">nodename</td>
<td>选取此节点的所有子节点</td>
<td>bookstore</td>
<td>选取bookstore下所有的子节点</td>
</tr>
<tr>
<td align="center">/</td>
<td>如果是在最前面，代表从根节点选取。否则选择某节点下的某个节点</td>
<td>/bookstore</td>
<td>选取根元素下所有的bookstore节点</td>
</tr>
<tr>
<td align="center">//</td>
<td>从全局节点中选取节点，随便在哪个位置</td>
<td>//book</td>
<td>从全局节点中找到所有的book节点</td>
</tr>
<tr>
<td align="center">@</td>
<td>选取某个节点的属性</td>
<td>//book[@price]</td>
<td>选取所有拥有price属性的book节点</td>
</tr>
</tbody></table>
<ol start="2">
<li><p>谓语：</p>
<p>谓语<strong>用来查找某个特定节点或者包含某个指定的值的节点</strong>，被嵌在方括号中。<br>下面的表格中，列出了带有位于的一些路径表达式以及表达式的结果</p>
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>/bookstore/book[1]</td>
<td>选取bookstore下的第一个子元素（下标从1开始）</td>
</tr>
<tr>
<td>/bookstore/book[last()]</td>
<td>选取bookstore下的倒数第二个book元素</td>
</tr>
<tr>
<td>bookstore/book[position()&lt;3]</td>
<td>选取bookstore下前面两个子元素</td>
</tr>
<tr>
<td>//book[@price]</td>
<td>选取拥有price属性的book元素</td>
</tr>
<tr>
<td>//book[@price=10]</td>
<td>选取所有属性price等于10的book元素</td>
</tr>
</tbody></table>
</li>
<li><p>通配符<br>*表示通配符</p>
<table>
<thead>
<tr>
<th>通配符</th>
<th>描述</th>
<th>示例</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>匹配任意节点</td>
<td>/bookstore/*</td>
<td>选取bookstore下的所有子元素</td>
</tr>
<tr>
<td>@*</td>
<td>匹配节点中的任何属性</td>
<td>//book[@*]</td>
<td>选取带有属性的book元素</td>
</tr>
</tbody></table>
</li>
<li><p>逻辑</p>
<p>|  &amp;  +  -  mod等等</p>
</li>
</ol>
</li>
</ul>
<pre><code>总结： 

* 注意/ 和// 的区别。是子节点和全部节点。 // 获取子孙节点。一般//用的多
* contains：有时候某个属性中包含了多个值，那么可以通过contains函数：
    例如：`//div[contains(@class, &quot;job_detail&quot;)]`
* 谓词下标是从1开始的</code></pre>
<ul>
<li><p><strong>lxml库</strong></p>
<p>使用lxml解析HTML代码：</p>
<ol>
<li><p>解析html字符串，使用<code>lxml.etree.HTML</code>进行解析。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">htmlElement = etree.HTML(text)</span><br><span class="line">print(etree.tostring(htmlElement, encoding=<span class="string">&quot;utf-8&quot;</span>).decode(<span class="string">&quot;utf-8&quot;</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>解析html文件。使用<code>lxml.etree.parse</code>进行解析。<br>注意，当文件中的html不规范的时候，注意更换解析器为HTMLParser。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parser = etree.HTMLParser(encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">htmlElement = etree.parse(<span class="string">&quot;tencent.html&quot;</span>, parser=parser)</span><br><span class="line">print(etree.tostring(htmlElement, encoding=<span class="string">&quot;utf-8&quot;</span>).decode(<span class="string">&quot;utf-8&quot;</span>))</span><br></pre></td></tr></table></figure>



</li>
</ol>
</li>
</ul>
<ul>
<li>lxml与XPath结合运用：<br>详见代码10</li>
</ul>
<h3 id="二-BeautifulSoup4库"><a href="#二-BeautifulSoup4库" class="headerlink" title="二. BeautifulSoup4库"></a>二. BeautifulSoup4库</h3><p>和lxml一样，BeautifuSoup也是一个HTML/XML的解析器。主要的功能也是如何解析和提取HTML/XML数据<br>lxml只会局部遍历。而BeautifulSoup是基于HTML DOM()的。会载入整个文档。解析整个DOM树，因此时间和内存开销大，性能低于lxml<br>但是非常简单。支持CSS选择器。和诸多解析器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span>  bs4 <span class="keyword">import</span> BeautifuSoup</span><br></pre></td></tr></table></figure>

<p>TIPS：    .prettify()       以更美观的方式打印</p>
<p><strong>TIPS :</strong><br><strong>string, strings, stripper_strings属性以及get_text方法：</strong></p>
<ol>
<li>string：获取某个标签下的非标签字符串。返回来的是个字符串。<font color='red'>如果字符有多行，则无法获得全部字符。</font></li>
<li>strings: 获取某个标签下的子孙非标签字符串。返回来的是一个生成器</li>
<li>stripper_strings: 获取某个标签下的子孙非标签字符串。会去掉空白字符。返回来的是一个生成器</li>
<li>get_text :获取某个标签下的子孙非标签字符串。不是以列表的形式返回，是以普通字符串返回</li>
</ol>
<p><strong>搜索文档树</strong></p>
<ol>
<li><p><strong>find和find_all 方法：</strong><br>find方法：找到第一个满足条件的标签后立即返回，只返回一个元素（<strong>tag类型</strong>，<strong>但打印的时候显示的是字符串</strong>（会自动调用repr方法））<br>find_all 方法：找到所有满足条件的标签。然后返回一个列表</p>
<ol>
<li>find_all在提取标签的时候，第一个参数是标签的名字，如果想用标签属性进行过滤的时候</li>
<li>有些时候，不想提取这么多。那么可以使用limit参数</li>
</ol>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">soup.find_all(<span class="string">&quot;a&quot;</span>, attrs=&#123;<span class="string">&quot;id&quot;</span>:<span class="string">&quot;link2&quot;</span>&#125;)</span><br><span class="line">soup.find_all(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;id&quot;</span>=<span class="string">&quot;link2&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>select 方法</strong><br>使用1方法可以方便的找出元素，但是有时候使用css选择器的方式可以更加方便。使用css选择器的语法，应该使用select方法。”.”表示查找”class”的属性，#表示查找id的属性。在规范的HTML中，相同的 <code>class</code> 可以出现在多个元素中，而一个 <code>id</code> 只能在一个元素中出现。</p>
<ol>
<li>通过标签名查找：<br><code>soup.select(&quot;a&quot;)</code></li>
<li>通过类名查找<br>通过类名，则应该在类的前面加一个<code>.</code><br><code>soup.select(&quot;&quot;.sister&quot;)</code></li>
<li>通过id查找：<br>通过id查找，则应该在id的名字前面加一个#号<br><code>soup.select(&quot;#link1&quot;)</code></li>
<li>组合查找。<ol>
<li>通过空格分开。会找到sister以内的所有子孙元素<br><code>soup.select(&quot;.sister #link1&quot;)</code><ol>
<li>通过 <code>&gt;</code> 分开。则只会找到sister的子元素</li>
<li>多个 <code>&gt;</code> 空格分开，可以查找对应的子元素</li>
</ol>
</li>
</ol>
</li>
<li>根据属性的名字进行查找：<br>应该先写标签名字。然后再在中括号中写属性的值<br><code>soup.select(input[name=&quot;name&quot;])</code><br>注意这里没有<font color='red'>@</font>, 与XPath区分一下。</li>
<li>在根据类名或者id进行查找的时候，如果还要根据标签名进行过滤。那么可以在类的前面或者id的前面加上标签名字：<br><code>soup.select(&quot;div.name&quot;)</code><br><code>soup.select(div#name)</code></li>
</ol>
</li>
</ol>
<p>TIPS: 详细的应用例子详见 代码</p>
<p><strong>Beautiful Soup 总结：</strong></p>
<ol>
<li><p><strong>四种对象：</strong></p>
<ol>
<li>Tag ： BeautifulSoup中所有的标签都是Tag类型。并且BeautifulSoup的对象本质上也是一个Tag类型。所以其实一些方法比如find，find_all并不是BeautifulSoup的。而是Tag的</li>
<li>NavigableString：继承自python中的str。用起来和python的str一样得</li>
<li>BeautifulSoup：继承自Tag。用来生成BeautifulSoup的。</li>
</ol>
</li>
<li><p>Comment：继承自NavigabelString</p>
</li>
<li><p><strong>contents和children：</strong></p>
<p>返回某个标签下的直接子元素。其中也包括字符串。<br>它们两个的区别是：contents返回来的是一个列表。children返回的是一个迭代器</p>
</li>
</ol>
<h3 id="三-正则表达式"><a href="#三-正则表达式" class="headerlink" title="三. 正则表达式"></a>三. 正则表达式</h3><p><strong>基本语法</strong></p>
<ol>
<li><code>re.match</code> 只能对字符串从头开始匹配，而<code>re.search</code> 能够对整个字符串进行匹配</li>
<li><code>.</code> 能够匹配除了<font color='red'>换行符</font>以外的任意一个字符</li>
<li><code>\d</code> 能够匹配任意一个数字，但<font color='red'>多位数</font>被认为含有多个数字，只会匹配第一个数字。如匹配 <code>18</code> 返回 <code>1</code> </li>
<li><code>\s</code> 能够匹配空白字符（\n，\t，\r，空格）</li>
<li><code>\w</code> 能够匹配 <code>a-z</code>,<code>A-Z</code>,数字和下划线</li>
<li><code>\W</code> 能够匹配 <code>/w</code> 匹配不到的字符，包括换行符等</li>
<li><code>[]</code> 只要满足中括号中的字符，就可以匹配。<code>[0-9]</code>表示匹配”0-9”的数字，而不是”0</li>
<li>“、”-“或”9”三个字符 。中括号中的 <code>^</code> 表示非</li>
</ol>
<p>具体案例</p>
<ol>
<li><p>验证手机号码：手机号码的规则是以 <code>1</code> 开头，第二位可以是 <code>34587</code>，后面那9位随意。示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">&#x27;18570631587&#x27;</span></span><br><span class="line">ret = re.match(<span class="string">&#x27;1[34587]\d&#123;9&#125;&#x27;</span>, text)</span><br><span class="line">print(ret.group())</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证邮箱：邮箱名称由 <code>字母、数字、下划线</code> 组成，然后是 <code>@</code> 符号，最后是域名。示例代码如下：</p>
</li>
</ol>
   <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">&#x27;tommyhechina@126.com&#x27;</span></span><br><span class="line">ret = re.match(<span class="string">&#x27;\w+@[0-9a-zA-Z]+\.[a-zA-Z]+&#x27;</span>, text)</span><br><span class="line">print(ret.group())</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>验证URL。URL的规则是前面是 <code>http</code>、<code>https</code> 或者 <code>ftp</code> ，然后加上 <code>:</code> ，然后加上 <code>//</code>，之后可以出现任意非空白字符。示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">&#x27;http://www.baidu.com/&#x27;</span></span><br><span class="line">ret = re.match(<span class="string">&#x27;(http|https|ftp)://[^\s]+&#x27;</span>, text)</span><br><span class="line">print(ret.group())</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="4">
<li><p>验证身份证：身份证的规则是，总共有18位，前17位都是数字，最后一位可以是数字，也可以是小写的x，也可以是大写的X。示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">&#x27;12345678901234567x&#x27;</span></span><br><span class="line">ret = re.match(<span class="string">&#x27;\d&#123;17&#125;[\d|x|X]&#x27;</span>, text)</span><br><span class="line">print(ret.group())</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p><strong>高级语法</strong>：</p>
<ol>
<li><code>^</code>（脱字号）：表示以…开始。如果在中括号中，表示取反操作</li>
<li><code>$</code> 表示以…结束。<code>$</code> 会识别左侧直接连接的非正则表达式字符串，或是一个正则表达式字符</li>
<li><code>|</code> 匹配多个表达式或字符串</li>
</ol>
<p>贪婪模式和非贪婪模式：贪婪模式尽可能多地匹配字符，非贪婪模式尽可能少地匹配字符。在正则表达式后加入 <code>?</code> 切换为非贪婪模式。在处理网页的时候，要注意切换为非贪婪模式</p>
<p>经典示例：</p>
<p>匹配0-100之间的数字，不允许出现09这样的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">&#x27;00&#x27;</span></span><br><span class="line">ret = re.match(<span class="string">&#x27;[1-9]\d?$|100$|0$&#x27;</span>, text)</span><br><span class="line">print(ret.group())</span><br></pre></td></tr></table></figure>

<p>转义字符和原生字符：在python和正则表达式中，<code>\</code> 均具有转义字符的含义，在正则表达式中使用原生字符串可以有效地降低复杂性，避免错误</p>
<p><strong>re模块中常用函数</strong></p>
<p>match：从开始的位置进行匹配。如果开始的位置没有匹配到，直接失败。</p>
<p>search：在字符串中寻找满足条件的字符。返回第一个满足条件的结果。</p>
<p>group：分组。可以使用<font color='red'>圆括号</font>对过滤得到的字符串进行分组。<code>group()</code> 等价于 <code>group(0)</code>，表示返回整个字符串；<code>groups</code> 返回里面的子组（以元组的形式），<font color='red'>索引从 <code>1</code> 开始</font>；<code>group(x)</code> 表示返回第 <code>x</code> 个子组；<code>group(x,y,z)</code> 表示返回第 <code>x</code>，<code>y</code>，<code>z</code>三个子组（以元组的形式）</p>
<p>findall：找出所有满足条件的字符，返回一个列表。如果在正则表达式中存在括号，则返回括号内的内容组成的列表</p>
<p>sub：将所有匹配到的字符串替换为其他字符串</p>
<p>split：使用匹配到的字符分割字符串，返回列表</p>
<p>compile：将常用的正则表达式进行编译，可以在多次访问中提高效率。尽量使用原生字符。引入re.VERBOSE参数可以输入多行的正则表达式以及添加注释。</p>
]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>爬虫笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫学习笔记——基础篇（三）</title>
    <url>/2020/08/16/1.3/</url>
    <content><![CDATA[<h1 id="爬虫学习笔记"><a href="#爬虫学习笔记" class="headerlink" title="爬虫学习笔记"></a>爬虫学习笔记</h1><h2 id="第三章-数据文件处理"><a href="#第三章-数据文件处理" class="headerlink" title="第三章 数据文件处理"></a>第三章 数据文件处理</h2><h3 id="一-json文件处理"><a href="#一-json文件处理" class="headerlink" title="一. json文件处理"></a>一. json文件处理</h3><ol>
<li><p><strong>JSON支持的数据格式：</strong></p>
<ul>
<li>对象(字典)： { }</li>
<li>数组(列表)： [ ]</li>
<li>整形，浮点型 </li>
<li>字符串类型，字符串必须要<font color='red'>双引号</font></li>
<li>多个数据之间使用逗号分开</li>
</ul>
</li>
</ol>
<p><strong>注意：JSON本质上就是一个字符串</strong></p>
<ol start="2">
<li><p>字典,列表,JSON字符串，JSON文件相互转换 （加了”s”和文件没关系，不加”s”和文件有关系）<br>主要用：dump,dumps,load,loads .</p>
<p><code>json.dump</code>：将json字符串写入文件中。注意，如果json字符中存在汉字，需要在打开文件的时候使用 <code>encoding=&#39;utf-8&#39;</code> 命令，以及在 <code>json.dump</code> 中设置 <code>ensure_ascii=False</code> 参数</p>
<p><code>json.dumps</code>：将python的字典、列表或字符串转换为json字符串。在python中，只有基本数据类型才能转换为json格式的字符串，即 <code>int</code>、<code>float</code>、<code>str</code>、<code>list</code>、<code>dict</code> 和 <code>turple</code></p>
<p><code>json.load</code>：读取json文件的内容，转换为列表。注意，如果json文件中包含汉字，需要在打开文件的时候使用 <code>encoding=&#39;utf-8&#39;</code> 命令</p>
<p><code>json.loads</code>：将json字符串转换为python的字典、列表或字符串</p>
</li>
</ol>
<h3 id="二-CSV文件处理："><a href="#二-CSV文件处理：" class="headerlink" title="二. CSV文件处理："></a>二. CSV文件处理：</h3><ol>
<li><p>CSV文件介绍</p>
<ol>
<li>纯文本，使用某个字符集等</li>
<li>由记录组成</li>
<li>每条记录被分隔符分割为字段（逗号，分号，空格，制表符）</li>
<li>每条记录都有同样的字段序列</li>
</ol>
</li>
<li><p>CSV读写操作：</p>
<p><strong>读取csv文件需要引入 <code>csv</code> 库</strong></p>
<p>两种读取 <code>csv</code> 文件的方法：</p>
<p>​    <code>with open(&#39;xxx.csv&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f</code> 代码可以准确读取 <code>csv</code> 文件中的中文</p>
<ol>
<li><p><code>reader = csv.reader(f)</code> ：</p>
<p><code>reader</code> 是一个迭代器，遍历 <code>reader</code> 每次返回一个列表。通过 <code>next(reader)</code> 命令可以去除 <code>reader</code> 迭代器的第 <code>0</code> 行，从第 <code>1</code> 行开始遍历。使用下标的方式获取对应的值，若 <code>csv</code> 文件结构发生改变，则获取的值发生改变，<font color='red'>不推荐</font></p>
</li>
<li><p><code>reader = csv.DictReader(f)</code></p>
<p><code>reader</code> 是一个迭代器，遍历 <code>reader</code> 每次返回一个有序字典。通过 <code>dict[&#39;key&#39;]</code> 命令获得对应的值，即使 <code>csv</code> 文件结构发生改变，获取的值也不会改变，推荐<font color='red'>使用这种方法</font></p>
</li>
</ol>
<p>两种写入 <code>csv</code> 文件的方法：</p>
<p>​    <code>with open(&#39;xxx.csv&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;, newline=&#39;&#39;) as f</code> 代码可以准确地将中文写入 <code>csv</code> 文件中， <code>newline=&#39;&#39;</code> 参数表示写入的每一行之间不需要空行，默认是 <code>\n</code>，即写入一行空一行</p>
<ol>
<li><p>以行的方式写入。<code>writerow</code> 表示每次写入一行，<code>writerows</code> 表示每次写入多行。这种写入方式的优点是<font color='red'>简单</font></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer = csv.writer(f)</span><br><span class="line">writer.writerow(monorow)</span><br><span class="line">writer.writerows(multirows)</span><br></pre></td></tr></table></figure>
</li>
<li><p>以字典的方式写入。注意，写入表头数据的时候，需要调用 <code>writeheader</code> 方法，表头不会自动写入。这种写入方式的优点是<font color='red'>准确</font>，即使写入的字典内数据的顺序发生改变，只要键值对依然正确，就可以按照表头的顺序正确写入数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">writer = csv.DictWriter(f, headers)</span><br><span class="line">writer.writerheader()</span><br><span class="line">writer.writerows(multirows)</span><br></pre></td></tr></table></figure>



</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>python爬虫</category>
      </categories>
      <tags>
        <tag>爬虫学习笔记</tag>
      </tags>
  </entry>
</search>
